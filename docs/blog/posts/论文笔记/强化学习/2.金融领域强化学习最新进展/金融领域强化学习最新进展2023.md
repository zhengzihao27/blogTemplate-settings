---
title: 金融领域强化学习最新进展
date: 
    created: 2024-09-27
    updated: 2024-09-27
authors: 
  - zzh
categories:
  - Reinforcement Learning
---

# Recent advances in reinforcement learning in finance (1)

一篇金融领域强化学习的综述（2021）。经典的随机控制理论在解决金融决策问题上严重依赖模型的假设，而强化学习不同。与经典的方法相比，他不需要太强的模型假设，但是需要大量的金融数据，才能在复杂的金融环境中改进决策。

<!-- more -->

**Autor：** Ben Hambly, Renyuan Xu, Huining Yang

## 2.强化学习基础

### 2.1 Setup:Markov decision processes

马尔科夫决策过程(MDPs, Markov decision processes)是具有马尔科夫性质的随机过程。

投资组合优化问题通常按时间是否有限去划分为两类：

1. Infinite time horizon 的 MDPs。主要研究长期的投资策略，如养老金问题。
2. finite time horizon 的 MDPs。主要研究短期内的交易，如资产购买或清算的最优执行问题，
若目标金额没有被完全执行，则可能会在终止时间受到处罚。

#### *Infinite time horizon and discounted reward.*

首先考虑**无限时间范围和折扣奖励的**的**离散时间MDP**。

* 状态空间 $\mathcal{S}$，Markov process取值的空间
* 动作集合 $\mathcal{A}$，通过在集合中采取动作影响演化
* 目的是通过选择一个策略（沿着时间顺序行动）来优化系统的预期回报。

通过定义 value function 来公式化该优化过程

***
Define value function $V^*$ for each $s \in \mathcal{S}$ to be

\begin{equation}
    V^{*}(s)=\sup _{\Pi} V^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \mid s_{0}=s\right],
\end{equation}

subject to

\begin{equation}
    s_{t+1} \sim P\left(s_{t}, a_{t}\right), \quad a_{t} \sim \pi_{t}\left(s_{t}\right) .
\end{equation}
***


以下做一些记号上的约定：

* $\mathbb{E}^{\Pi}$ 表示在策略 ${\Pi}$ 下的期望，其中概率测度 $\mathcal{P}$ 描述MDP中的动态和奖励。
* 一个空间 $\mathcal{X}$ 的概率测度写为 $\mathcal{P}(\mathcal{X})$ 
* 状态空间 $(\mathcal{S},d_\mathcal{S})$ 和动作空间 $(\mathcal{A},d_\mathcal{A})$ 都是完全可分的度量空间，包括 $\mathcal{S}$ 和 $\mathcal{A}$ 都是有限的情况，这在RL文献中常见。
* $\gamma \in (0,1)$ 是一个折现因子
* $s_t \in \mathcal{S}$：$t$ 时刻的状态
* $a_t \in \mathcal{A}$：$t$ 时刻的动作
* $P: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(S)$ 马尔可夫过程下的转移函数
* 记号 $s_{t+1} \sim P\left(s_{t}, a_{t}\right)$ 表示 $s_{t+1}$ 从分布 $P(s_t,a_t)\in \mathcal{P}(\mathcal{S})$ 中进行采样。
* 奖励 $r(s,a)$ 是每对$(s,a)\in \mathcal{S}\times \mathcal{A}$ 的 $\mathbb{R}$ 中的一个随机变量
* 策略 $\Pi=\left\{\pi_{t}\right\}_{t=0}^{\infty}$ 是马尔可夫的，因为它只依赖于当前状态，并且可以确定性的，也可以是随机的。
:   * 对于确定性的策略，$\pi_{t}: \mathcal{S} \rightarrow \mathcal{A}$ 将当前状态 $s_t$ 映射到一个确定性的动作
:   * 对于随机策略，$\pi_{t}: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ 将当前状态 $s_t$ 映射到一个动作空间上的分布。
:   &emsp;&emsp;&emsp; $\pi_t(s)\in\mathcal{P}(\mathcal{A})$ 表示给定状态 $s$ 在行动空间上的分布
:   &emsp;&emsp;&emsp; $\pi_t(s,a)$ 表示在状态 $s$ 采取动作 $a$ 的概率

MDPs With infinite-time horizon。在关于 MDPs 的文献中，通常都假设了奖励 $r$ 和转移动态 $P$ 是时间齐次(time homogeneous)的。

此外，在问题的设置上，最大化或者最小化优化目标，本质上都是一致的。

> 动态规划原则(dynamic programming principle, DPP)：最优策略可以通过最大化单步的奖励，然后在新的状态下进行最优化处理，来得到递归方程。

我们可以用用动态规划来推导值函数(1)的贝尔曼方程

\begin{equation}
    V^{*}(s)=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right]\right\} .
\end{equation}

引入 Q-function ，我们可以将值函数改写为

\[V^{*}(s)=\sup _{a \in \mathcal{A}} Q^{*}(s, a) ,\]

??? Q-function
    Q-function 是强化学习中一个基本量，其定义如下：

    \begin{equation}
        Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right],
    \end{equation}

    可以理解为在状态 $s$ 下采取动作 $a$ 的预期奖励，在此之后遵循最优策略。
    
对于 Q-function，也可以写出相应的贝尔曼方程，如下：

\begin{equation}
    Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)} \sup _{a^{\prime} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime}\right) .
\end{equation}

这使我们能够从 $Q(s,a)$ 中提取最优（平稳）策略
$\pi^*(s,a)$（如果存在的话），具体为

\[\pi^{*}(s, a) \in \arg \max _{a \in \mathcal{A}} Q(s, a) .\]

该无限时间问题采取的奖励 reward 设置，对未来的奖励 reward 进行贴现。还有另一种设置是将奖励 reward 进行平均，这也被称为遍历式奖励，但是这种遍历式的奖励通常与金融应用不相关。

#### *Finite time horizon.*

有限时间范围 $T<\infty$ ，不再贴现未来的价值并且有一个终止奖励。有限时间范围的MDP问题可以表示如下：

\begin{equation}
    V_{t}^{*}(s)=\sup _{\Pi} V_{t}^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{u=t}^{T-1} r_{u}\left(s_{u}, a_{u}\right)+r_{T}\left(s_{T}\right) \Bigg| s_{t}=s\right], \forall s \in \mathcal{S},
\end{equation}

subject to

\begin{equation}
    s_{u+1} \sim P_{u}\left(s_{u}, a_{u}\right), \quad a_{u} \sim \pi_{u}\left(s_{u}\right), \quad t \leq u \leq T-1.
\end{equation}

在无限时间情况的例子中，我们用 $s_u\in \mathcal{S}$ 和 $a_u\in \mathcal{A}$ 表示 agent 在时间 $u$ 的状态和动作。
然而，有限时间与无限时间有一点不同的是，我们允许时间依赖性的转移和奖励函数。

* $P_{u}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(S)$ 表示转移函数
* $r_u(s,a)$ 对每一对$(s,a)\in \mathcal{S}\times \mathcal{A}$ 是一个实值的随机变量，$t\leq u \leq T-1$
* $r_T(s)$ 终端奖励，对所有的 $s\in\mathcal{S}$ 是一个实值随机变量
* $\Pi=\left\{\pi_{u}\right\}_{t=0}^{T}$ 马尔可夫策略可以是确定性的也可以是随机的

值函数(6)对应的贝尔曼方程定义如下：

\begin{equation}
    \begin{split}
        V_{t}^{*}(s) &=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t+1}^{*}\left(s^{\prime}\right)\right]\right\}, \\
        V_{T}^{*}(s) &=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{(terminal condition)}
    \end{split}
\end{equation}

可以将值函数写成

\[V_{t}^{*}(s)=\sup _{a \in \mathcal{A}} Q_{t}^{*}(s, a),\]

其中 $Q_t^*$ 函数定义如下：

\begin{equation}
    Q_{t}^{*}(s, a)=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t}^{*}\left(s^{\prime}\right)\right].
\end{equation}

$Q$-function 的贝尔曼方程由下式给出

\begin{equation}
    \begin{split}
        Q_{t}^{*}(s, a) &=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[\sup _{a^{\prime} \in \mathcal{A}} Q_{t+1}^{*}\left(s^{\prime}, a^{\prime}\right)\right],\\
        Q_{T}^{*}(s, a) &=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{for all } a \in \mathcal{A}
    \end{split}
\end{equation}

??? Note
    金融时间序列数据通常是非平稳的，因此(6)和(7)式中的时间变化转移核和价格函数对于金融应用尤为重要。

对于一个具有有限状态 **(finite state)** 和动作 **(finite action)** 空间以及有限奖励 $r$ **(finite reward)** 的无限时间范围 **(infinite time)** 的马尔可夫决策过程（MDP），只要存在最优策略，该 MDP 总是具有一个平稳的最优策略 (stationary optimal policy)。

> 在一个无限时间跨度的马尔可夫决策过程（MDP）中，如果最优策略存在，那么就可以找到一个静态最优策略，即一个与时间无关的策略，使得在任何时间点应用它都能达到最优结果。即策略不需要随着时间变化。

***
**Theorem 2.1** Theorem 6.2.7 in Puterman (2014). Assume $|\mathcal{A}|<\infty$, 
$|\mathcal{S}|<\infty$, and $|r|<\infty$ with
probability one. For any infinite horizon discounted MDP, 
there always exists a deterministic stationary policy that is optimal.

***

定理 2.1 意味着总是存在一个固定的策略，可以在每个时间步长执行该策略指定的操作最大化折扣奖励。
agent 不需要随时间改变策略。平均奖励的情况也有一个类似的结果，见 Theorem 8.1.2 in Puterman (2014).

这将寻找最优顺序决策策略问题减少到寻找最优平稳策略的问题。因此，记 $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ (没有时间指数) 为一个平稳策略。

#### *Linear MDPs and linear functional approximation.*

!!! Note
    在线性马尔可夫决策过程（MDP）中，转移核（transition kernels）和奖励函数（reward function）被假设为相对于某些特征映射是线性的（Bradtke & Barto, 1996；Melo & Ribeiro, 2007）。

在**无限时间范围的环境**下，若存在 $d$ 个在 $\mathcal{S}$ 上未知(符号)测度 $\mu=\left(\mu^{(1)}, \ldots, \mu^{(d)}\right)$ ，以及一个未知的向量 $\theta\in\mathbb{R}^d$，满足对任意的 $(s,a)\in \mathcal{S}\times\mathcal{A}$，有

\begin{equation}
    P(\cdot \mid s, a)=\langle\phi(s, a), \mu(\cdot)\rangle, \quad r(s, a)=\langle\phi(s, a), \theta\rangle .
\end{equation}

则称该MDP为具有特征映射的特征映射 $\phi:\mathcal{S}  \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 的**线性MDP**。

与之相似，在**有限时间环境**下，若对于任意的 $0\leq t \leq T$，存在 $d$ 个在 $\mathcal{S}$上的 未知(符号)测度 $\mu_{t}=\left(\mu_{t}^{(1)}, \ldots, \mu_{t}^{(d)}\right)$
以及一个未知的向量 $\theta_t \in\mathbb{R}^d$，
满足对任意的 $(s,a)\in \mathcal{S}\times\mathcal{A}$，有

\begin{equation}
    P_{t}(\cdot \mid s, a)=\left\langle\phi(s, a), \mu_{t}(\cdot)\right\rangle, \quad r_{t}(s, a)=\left\langle\phi(s, a), \theta_{t}\right\rangle
\end{equation}

则称该MDP为具有特征映射的特征映射 $\phi:\mathcal{S}  \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 的**线性MDP**。

通常假设这些特征(features)是 agent 已知且有界的，也就是说 $\|\phi(s, a)\| \leq 1$ 对所有的 $(s,a)\in\mathcal{S}\times\mathcal{A}$。

线性MDP框架与具有线性函数逼近的RL的文献密切相关，其中的值函数假设为形式

For the infinite horizon case,

\begin{equation}
    Q(s, a)=\langle\psi(s, a), \omega\rangle, \quad V(s)=\langle\xi(s), \eta\rangle
\end{equation}


For the finite horizon case,

\begin{equation}
    Q_{t}(s, a)=\left\langle\psi(s, a), \omega_{t}\right\rangle, \quad V_{t}(s)=\left\langle\xi(s), \eta_{t}\right\rangle, \forall 0 \leq t \leq T
\end{equation}

* $\psi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d}$ 和 $\xi: \mathcal{S} \rightarrow \mathbb{R}^{d}$ 是已知的特征映射
* $\omega, \omega_t,\eta$ 和 $\eta_t$ 是未知向量

在温和条件下，线性马尔可夫决策过程(即(11)或(12))与
线性函数逼近形式(即(13)或(14))是等价的（Jin et al., 2020；Yang & Wang, 2019）。


#### *Nonlinear functional approximation.*

与线性函数逼近相比，非线性函数逼近不需要先验的 kernel functions 知识。
非线性函数有可能解决当 agent 对MDP所处的函数空间理解不正确，引起的模型误设问题。
最流行的的非线性函数逼近方法是使用神经网络，从而形成深度强化学习(deep RL)。
得益于`Universal approximation theorem`定理，这是一种理论上合理的方法，
且神经网络在广泛的应用中表现出良好的性能。
同时，对于某些神经网络架构，基于梯度的算法具有可证明的收敛性保证。

### 2.2 From MDP to learning

当转移动态 $P$ 和奖励函数 $r$ 对于无限时间范围的 MDP 问题是未知的，该 MDP 问题就会变为 RL 问题
（即寻找一个最优的平稳策略 $\pi$ （如果存在），同时学习未知的 $P$ 和 $r$ ）。
$P$ 和 $r$ 的学习可以是显示的，也可以是隐式的，这分别形成了 model-based 和 model-free RL.
类似的思想也适用于有限时间范围。接下来引入一些标准的 RL 术语。

##### *Agent-environment interface.*

:   在强化学习（RL）环境中，学习者或决策者称为智能体（agent）。agent 所操作和互动的物理世界，包括 agent 之外的所有事物，称为环境（environment）。
agent 和 environment 在一系列离散时间步骤 $t=0,1,2,\dots,$ 中以以下方式进行互动。

* At the beginning of each time step $t$, agent 接受 environment 状态的某种表示 $s_t\in\mathcal{S}$，并选择一个动作 $a_t\in\mathcal{A}$。
* At the end of this time step, 作为其动作产生的部分结果，agent 从 environment 接收到一个数值奖励 $r_t$（可能是随机的）和一个新状态 $s_{t+1}$。

:   $(s_t,a_t,r_t,s_{t+1})$ 称为时间 $t$ 的一个样本，而 $h_{t}:=\left\{\left(s_{u}, a_{u}, r_{u}, s_{u+1}\right)\right\}_{u=0}^{t}$
被称为截至时间 $t$ 的历史或经验。强化学习算法是 agent 与 environment 互动的良定策略的有限序列。

##### *Exploration versus exploitation.*

:   为了最大化时间积累的奖励，agent 需要做出一系列动作。  
    :   利用 (exploitation): agent 基于过去的经验进行学习并选择动作。  
    探索 (Exploration): agent 尝试做出新的选择。
:   探索的好处在于能够提供了将当前次优解改进为全局最优解的机会，但这需要耗费时间和计算资源，过度探索可能会削弱算法收敛到最优解的速度。
而不考虑探索的纯利用学习过程，仅基于过去经验短视地选择当前的动作，
虽然易于实现，但却往往导致次优的全局解。
因此，在强化学习算法的设计中，为提升学习和优化表现，需要适当平衡探索与利用。

##### *Simulator.*

:   上述描述的 agent 与 environment 的交互过程，agent 是以在线方式与 nvironment 进行交互的。
即，在时间步骤 $t+1$ 时的初始状态 $s_{t+1}$ 是 agent 从状态 $s_t$ 采取动作 $a_t$
后移动到的状态。
这是一个具有挑战性的设置，因为需要有效的探索方案。例如，使用 $\epsilon$-greedy 
策略的 $Q$-learning 可能需要经历指数级的时间才能学习到最优策略。
一些研究假设可以访问模拟器，允许算法查询任意的状态-动作对并返回奖励和下一个状态。
这意味着智能体可以在任何时候“重启”系统。也就是说，在时间步骤 $t+1$ 时的初始状态
不需要是智能体从状态 $s_t$ 采取动作 $a_t$ 后移动到的状态。
这个“Simulator”显著减轻了探索的难度，
因为一种简单的探索策略，均匀随机查询所有状态-动作对，
已经能成为寻找最优策略的最有效算法。

##### *Randomized policy versus deterministic policy.*

:   随机策略 $\pi_{t}: \mathcal{S}  \rightarrow \mathcal{P}(\mathcal{A})$
在控制文献中也被称为松弛控制，
在博弈论中也被称为混合策略。
尽管在定理2.1中存在一个确定性的最优策略，
但当 RL agents 对环境不确定时，大多数RL算法都采用随机策略来鼓励探索。

##### *An example: multi-armed bandits.*

:   多臂老虎机模型


#### 2.2.1 Performance evaluation

一般是不可能解析求解MDPs的，因此我们将讨论数值算法来确定最优策略。
为了评估不同的算法，我们需要一个衡量它们的性能。这里我们将考虑几种类型的性能度量：
样本复杂度、收敛速度、regret分析和渐近收敛性。

* 对于有限时间的RL，一个 episode 包含一系列从0开始到时间T的状态、动作、和奖励，用所有 episodes 的样本总数去评估性能。通常称为 episodic RL。  
* 对于无限时间的RL，以步数去评估性能。一步包含一个 (状态，动作，奖励，下一个状态) 的元组。

> 值得注意的是这种 episodic 准则也可以用来分析无限时间问题。一种流行的做法是在最大时间截断轨迹，将问题转化为近似的有限时间问题。

[TODO]

#### 2.2.2 Classification of RL algorithms

一个 RL 算法包含以下一个或多个部件：

* 值函数的表示，提供每个状态或每个状态/动作对有多好的预测。
* 策略 $\pi(s)$ 或 $\pi(s,a)$ 的直接表示
* 一种环境模型（估计的转移函数和估计的奖励函数），并结合一种规划算法
（使用模型来创建或改进策略的任何计算过程）。

前两个组成部分与所谓的无模型强化学习（model-free RL）相关。
当使用后者组件时，算法被称为基于模型的强化学习（model-based RL）。

在马尔可夫决策过程（MDP）设置中，
model-based 算法通过估计转移概率和奖励函数来维持一个近似的 MDP 模型，并
从近似的 MDP 中推导出价值函数。然后，从价值函数中导出策略。另一类基于模型的算法则是
对模型进行结构假设，使用一些先验知识，并利用结构信息进行算法设计。

与 model-based 的方法不同，model-free 算法直接学习价值（或状态-价值）函数或最优策略，
而无需推断模型。model-free 算法可以进一步分为两类：value-based 和 policy-based 的方法。

:   policy-based 方法显式构建策略的表示，并在学习过程中将其保存在内存中。
例子包括策略梯度方法和信任区域策略优化(trust region policy optimization, TRPO)方法。

:   value-based 作为一种替代方法，在学习过程中仅存储一个价值函数，
而不显式存储策略。在这种情况下，策略是隐式的，可以直接从价值函数中推导（通过选择具有最佳价值的动作）。

### 2.3 Value-based methods

:   **Setting:** Infinite time horizon with discounting, finite state and action space $|\mathcal{A}|<\infty$ and $|\mathcal{S}|<\infty$, and stationary policies.

#### 2.3.1 Temporal-difference learning

通过策略 $\pi$ 获得一些样本 $(s,a,r,s')$，agent 可以在 $(n+1)th$ 迭代时更新对值函数 $V^\pi$ (defined in Equation(1)) 的估计。

\begin{equation}
    V^{\pi,(n+1)}(s) \leftarrow\left(1-\beta_{n}(s, a)\right) \underbrace{V^{\pi,(n)}(s)}_{\text {current estimate }}+\beta_{n}(s, a)[\underbrace{r+\gamma V^{\pi,(n)}\left(s^{\prime}\right)]}_{\text {new estimate }},
\end{equation}

* $V^{\pi,(0)}$ 值函数的初始化
* $\beta_n(s,a)$ (1) 是第 $n+1$ 迭代时的学习速率，平衡了当前估计和新估计之间的权重。
    { .annotate }

    1.  $\beta_n$ 可以是一个常数，也可以依赖于当前状态 $s$，甚至可以依赖于迭代至 $n+1$ 次时，已观测到的样本。

以下是该算法的伪代码，该公式也被称为 TD(0) method，或者 one-step TD method。

**Algorithm 1** TD(0) Method for estimating $V^\pi$  
***
**Input:** total number of iterations $N$; the policy $\pi$ used to sample
observations; rule to set learning rate $\beta_n \in (0,1](0\leq n \leq N-1)$  
Initialize $V^{\pi,(0)}(s)$ for all $s\in \mathcal{S}$  
Initialize $s$  
**for** $n=0,\dots,N-1$ **do**  
&emsp;Sample action $a$ according to $\pi(s)$  
&emsp;Observe $r$ and $s'$ after taking action $a$  
&emsp;Update $V^{\pi,(n+1)}$ according to (2.21) with $(s,a,r,s')$  
&emsp;$s\gets s'$  
**end for**
***

??? Note
    TD($\lambda$) method 结合了两种常见的强化学习方法：**时序差分学习 (TD learning)**(权重 $\lambda$ ) 和**Monte Carlo method** (权重 $1-\lambda$)。  

    1. TD learning：逐步更新的强化学习方法。它通过对当前状态和未来状态的估计差（时间差分）进行更新。TD 学习适合在没有完整轨迹信息的情况下进行学习。  
    2. 蒙特卡洛方法：基于整个轨迹的更新方法，必须等到一个完整的轨迹结束之后，才能根据实际的回报来更新估计。它可以使用更多历史信息进行更新。  

    TD($\lambda$) 使用权重参数 $\lambda$ 来平衡两种方法:

    * 当 $\lambda = 1$ 时，TD$(\lambda)$ 变为蒙特卡洛方法，因为它完全依赖于完整的轨迹。  
    * 当 $\lambda = 0$ 时，TD(𝜆) 变为普通的 TD learning，因为它只使用单步更新。  
    * $\lambda$ 介于 0 和 1 之间时，它是两种方法的组合，既利用了时间差分学习的快速更新，又利用了蒙特卡洛方法对长期回报的考虑。  
    
    简单来说，TD$(\lambda)$是一个权衡短期更新和长期更新的混合方法。
    
TD 更新方程可以重写为：

\begin{equation}
    V^{\pi,(n+1)}(s) \leftarrow V^{\pi,(n)}(s)+\beta_{n}(s, a) \left[\underbrace{\overbrace{r+\gamma V^{\pi,(n)}\left(s^{\prime}\right)}^{\mathrm{TD} \text { target }}-V^{\pi,(n)}(s)}_{\text {TD error } \delta_{n}}\right] .
\end{equation}

* $\delta_n$: TD error ，测量在当前状态 $s$ 的估计值好更好的估计(TD target) $r+\gamma V^{\pi,(n)}(s')$ 之间的差值。
* TD error 和TD target 是分析近似值函数收敛性的两个重要组成部分


#### 2.3.2 Q-learning algorithm

[TODO]